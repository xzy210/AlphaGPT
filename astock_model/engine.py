"""
Aè‚¡è®­ç»ƒå¼•æ“

ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒ AlphaGPT
"""
import torch
import json
from torch.distributions import Categorical
from tqdm import tqdm
from loguru import logger

from .alphagpt import AStockAlphaGPT, NewtonSchulzLowRankDecay, StableRankMonitor
from .vm import AStockStackVM
from .backtest import AStockBacktest
from .data_loader import AStockDataLoader


class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ batch size
    # 10GB æ˜¾å­˜å»ºè®® 256-512ï¼Œ8GB æ˜¾å­˜å»ºè®® 128-256
    BATCH_SIZE = 256
    
    TRAIN_STEPS = 1000
    MAX_FORMULA_LEN = 16
    D_MODEL = 128
    
    # ä¿å­˜è·¯å¾„
    BEST_STRATEGY_PATH = "best_astock_strategy.json"
    TRAINING_HISTORY_PATH = "astock_training_history.json"


class AStockAlphaEngine:
    """Aè‚¡è®­ç»ƒå¼•æ“"""
    
    def __init__(self, use_lord: bool = True, lord_decay_rate: float = 1e-3):
        """
        åˆå§‹åŒ–è®­ç»ƒå¼•æ“
        
        Args:
            use_lord: æ˜¯å¦ä½¿ç”¨ LoRD æ­£åˆ™åŒ–
            lord_decay_rate: LoRD è¡°å‡ç‡
        """
        # åŠ è½½æ•°æ®
        logger.info("æ­£åœ¨åŠ è½½æ•°æ®...")
        self.loader = AStockDataLoader(device=str(ModelConfig.DEVICE))
        # min_days=20: åªè¦æœ‰20å¤©æ•°æ®å°±å¯ä»¥è®­ç»ƒ (é€‚åˆåˆšåŒæ­¥çš„æ•°æ®)
        self.loader.load_data(limit_stocks=500, lookback_days=250, min_days=20)
        
        if self.loader.feat_tensor is None:
            raise ValueError("æ•°æ®åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥:\n"
                           "  1) æ˜¯å¦è¿è¡Œäº†æ•°æ®ç®¡çº¿: python run_astock_pipeline.py\n"
                           "  2) MiniQMT æ˜¯å¦å¯åŠ¨å¹¶ç™»å½•\n"
                           "  3) æ•°æ®åº“ daily_kline è¡¨æ˜¯å¦æœ‰æ•°æ®")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = AStockAlphaGPT(
            d_model=ModelConfig.D_MODEL,
            max_formula_len=ModelConfig.MAX_FORMULA_LEN
        ).to(ModelConfig.DEVICE)
        
        # ä¼˜åŒ–å™¨
        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)
        
        # LoRD æ­£åˆ™åŒ–
        self.use_lord = use_lord
        if use_lord:
            self.lord_opt = NewtonSchulzLowRankDecay(
                self.model.named_parameters(),
                decay_rate=lord_decay_rate,
                num_iterations=5,
                target_keywords=["attention", "q_proj", "k_proj"]
            )
            self.rank_monitor = StableRankMonitor(self.model)
        else:
            self.lord_opt = None
            self.rank_monitor = None
        
        # è™šæ‹Ÿæœºå’Œå›æµ‹
        self.vm = AStockStackVM()
        self.bt = AStockBacktest()
        
        # è®°å½•
        self.best_score = float('-inf')
        self.best_formula = None
        self.training_history = {
            'step': [],
            'avg_reward': [],
            'best_score': [],
            'stable_rank': []
        }
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("=" * 50)
        logger.info("ğŸš€ Aè‚¡ AlphaGPT è®­ç»ƒå¼€å§‹")
        if self.use_lord:
            logger.info("   LoRD æ­£åˆ™åŒ–å·²å¯ç”¨")
        logger.info("=" * 50)
        
        pbar = tqdm(range(ModelConfig.TRAIN_STEPS), desc="Training")
        
        for step in pbar:
            bs = ModelConfig.BATCH_SIZE
            
            # åˆå§‹è¾“å…¥ (èµ·å§‹ Token)
            inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)
            
            log_probs = []
            tokens_list = []
            
            # è‡ªå›å½’ç”Ÿæˆå…¬å¼
            for _ in range(ModelConfig.MAX_FORMULA_LEN):
                logits, _, _ = self.model(inp)
                dist = Categorical(logits=logits)
                action = dist.sample()
                
                log_probs.append(dist.log_prob(action))
                tokens_list.append(action)
                
                inp = torch.cat([inp, action.unsqueeze(1)], dim=1)
            
            # ç»„åˆå…¬å¼
            seqs = torch.stack(tokens_list, dim=1)  # [Batch, SeqLen]
            
            # è¯„ä¼°å¥–åŠ±
            rewards = torch.zeros(bs, device=ModelConfig.DEVICE)
            
            for i in range(bs):
                formula = seqs[i].tolist()
                
                # æ‰§è¡Œå…¬å¼
                result = self.vm.execute(formula, self.loader.feat_tensor)
                
                if result is None:
                    rewards[i] = -5.0
                    continue
                
                # æ£€æŸ¥å˜å¼‚æ€§
                if result.std() < 1e-4:
                    rewards[i] = -2.0
                    continue
                
                # å›æµ‹è¯„ä¼°
                score, ret_val = self.bt.evaluate(
                    result, 
                    self.loader.raw_data_cache, 
                    self.loader.target_ret
                )
                rewards[i] = score
                
                # è®°å½•æœ€ä½³
                if score.item() > self.best_score:
                    self.best_score = score.item()
                    self.best_formula = formula
                    
                    formula_str = self.vm.decode_formula(formula)
                    tqdm.write(f"[!] æ–°æœ€ä¼˜: Score={score:.3f} | Ret={ret_val:.2%}")
                    tqdm.write(f"    å…¬å¼: {formula_str}")
            
            # è®¡ç®—ä¼˜åŠ¿
            adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
            
            # ç­–ç•¥æ¢¯åº¦æŸå¤±
            loss = 0
            for t in range(len(log_probs)):
                loss += -log_probs[t] * adv
            loss = loss.mean()
            
            # åå‘ä¼ æ’­
            self.opt.zero_grad()
            loss.backward()
            self.opt.step()
            
            # åº”ç”¨ LoRD
            if self.use_lord:
                self.lord_opt.step()
            
            # è®°å½•
            avg_reward = rewards.mean().item()
            postfix = {'AvgRew': f"{avg_reward:.3f}", 'Best': f"{self.best_score:.3f}"}
            
            if self.use_lord and step % 100 == 0:
                rank = self.rank_monitor.compute()
                postfix['Rank'] = f"{rank:.2f}"
                self.training_history['stable_rank'].append(rank)
            
            self.training_history['step'].append(step)
            self.training_history['avg_reward'].append(avg_reward)
            self.training_history['best_score'].append(self.best_score)
            
            pbar.set_postfix(postfix)
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        logger.info("=" * 50)
        logger.info("âœ… è®­ç»ƒå®Œæˆ!")
        logger.info(f"   æœ€ä¼˜å¾—åˆ†: {self.best_score:.4f}")
        logger.info(f"   æœ€ä¼˜å…¬å¼: {self.vm.decode_formula(self.best_formula)}")
        logger.info("=" * 50)
    
    def _save_results(self):
        """ä¿å­˜ç»“æœ"""
        # ä¿å­˜ç­–ç•¥
        strategy_data = {
            'formula': self.best_formula,
            'formula_str': self.vm.decode_formula(self.best_formula),
            'score': self.best_score,
        }
        
        with open(ModelConfig.BEST_STRATEGY_PATH, 'w', encoding='utf-8') as f:
            json.dump(strategy_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç­–ç•¥å·²ä¿å­˜åˆ°: {ModelConfig.BEST_STRATEGY_PATH}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(ModelConfig.TRAINING_HISTORY_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {ModelConfig.TRAINING_HISTORY_PATH}")


def train_astock():
    """è®­ç»ƒAè‚¡ç­–ç•¥"""
    engine = AStockAlphaEngine(use_lord=True, lord_decay_rate=1e-3)
    engine.train()


if __name__ == "__main__":
    train_astock()

